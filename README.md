# Отчет по третьей лабораторной работе

**1. Цель и постановка задачи**

Целью данной работы является реализация упрощённой трансформерной языковой модели (tiny-GPT) «с нуля» и её обучение на корпусе русской поэзии. В рамках работы необходимо самостоятельно собрать текстовый корпус, выполнить его предварительную обработку и токенизацию, реализовать архитектуру автогрессионной трансформерной модели в библиотеке PyTorch, обучить модель и проанализировать результаты генерации.

В качестве обучающих данных используются стихотворения классиков русской поэзии: А. С. Пушкина, М. Ю. Лермонтова и Ф. И. Тютчева. Основной задачей модели является предсказание следующего токена по предыдущему контексту, что позволяет использовать её для генерации поэтического текста.

**2. Теоретические основы**

**2.1 Автрорегрессионные языковые модели**

Языковая модель – это статистическая или нейросетевая модель, оценивающая вероятность последовательности токенов. Для последовательности токенов (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>T</sub>) вероятность можно разложить как произведение условных вероятностей:

Px1,x2,…, xP=t=1TPxtx1,…, xt-1

Автрорегрессионные модели, такие как GPT, реализуют именно такую формулу: каждый токен предсказывается на основе всех предыдущих токенов. Это делает модель подходящей для генерации текста, автодополнения и создания диалоговых систем.

Для обучения используется **кросс-энтропийная функция потерь**, которая измеряет разницу между предсказанной вероятностью токена и его истинной меткой. Минимизация этой функции эквивалентна максимизации вероятности правильных последовательностей токенов в обучающем корпусе.

Таким образом, модель учится предсказывать следующий токен так, чтобы сгенерированный текст имел вероятностную структуру, максимально близкую к обучающему корпусу.

**2.2 Архитектура Transformer и TinyGPT**

В отличие от классических RNN (рекуррентных сетей), которые читают текст последовательно, Transformer обрабатывает весь текст **параллельно**. Основная идея – использовать механизм внимания (attention), который позволяет каждому слову в тексте «смотреть» на другие слова и учитывать их влияние.

**Self-Attention**

Self-attention вычисляет для каждого токена три вектора:

- **Query (Q)** — что мы ищем;
- **Key (K)** — что у нас есть;
- **Value (V)** — что мы отдаем.

Затем внимания измеряет «взаимосвязь» между токенами:

Attention(Q,K,V)=softmax(QKTdk)V

Это означает, что модель смотрит на все позиции в тексте и вычисляет, насколько каждый токен важен для предсказания следующего.

**2.3 Tokenization и Byte Pair Encoding (BPE)**

Нейросети работают с числами, поэтому текст необходимо преобразовать в последовательность токенов. В TinyGPT используется метод **Byte Pair Encoding (BPE)**. Алгоритм BPE начинается с символьной токенизации и итеративно объединяет наиболее частотные пары символов или подслов, создавая словарь фиксированного размера.

Преимущества BPE:

- уменьшение количества редких токенов, что повышает эффективность обучения;
- баланс между символьной и словарной токенизацией, что позволяет корректно обрабатывать новые или редкие слова;
- компактный словарь фиксированного размера, что удобно для реализации модели на ограниченных ресурсах.

Для поэзии отдельный токен создаётся для символа перевода строки \n, чтобы модель могла различать строки и строить структуру стихотворения.

**2.4 Обучение и функции потерь**

Процесс обучения TinyGPT основан на минимизации **кросс-энтропийного loss**, который сравнивает предсказанные вероятности каждого токена с истинными метками. На практике это означает, что модель учится предсказывать следующий токен в последовательности так, чтобы вероятность правильного токена была максимально высокой.

Для оценки качества модели используется **perplexity (PPL)** — показатель, который измеряет, насколько хорошо модель предсказывает текст. Перплексия рассчитывается как экспонента от значения кросс-энтропии:

PPL=e<sup>Loss</sup>

Низкая perplexity означает, что модель делает более точные предсказания и лучше отражает структуру и стиль корпуса. В ходе обучения метрики loss и perplexity вычисляются как на обучающем наборе, так и на валидационном, что позволяет отслеживать переобучение.

**2.5 Методы регуляризации и предотвращения переобучения**

Поскольку корпус поэзии небольшой, важно предотвратить переобучение. В TinyGPT применяются следующие методы:

- **Dropout**: случайное зануление части нейронов на этапе обучения, что предотвращает слишком сильное запоминание обучающего набора;
- **Early Stopping**: прекращение обучения, если метрика perplexity на валидации не улучшается несколько шагов подряд. Это позволяет сохранить модель в состоянии наилучшей обобщающей способности.

**2.6 Генерация текста**

После обучения модель способна генерировать новые стихотворения автрорегрессионно: на каждом шаге она предсказывает следующий токен, добавляет его к последовательности и повторяет процесс.

Для улучшения качества и разнообразия генерации используются:

- **Temperature sampling**: масштабирует логиты перед softmax, контролируя «остроту» распределения. Низкая температура делает текст более предсказуемым, высокая — увеличивает разнообразие.
- **Top-k sampling**: ограничивает выбор следующего токена k наиболее вероятными вариантами, что уменьшает риск генерации редких и неуместных токенов.

Эти методы позволяют модели создавать более осмысленные и разнообразные стихотворные тексты.

**3. Реализация и описание кода**
### **3.1 Сбор корпуса русской поэзии**
Для обучения модели сначала нужно собрать тексты стихотворений. Мы скачиваем их с сайта **stihi-rus.ru** для трёх авторов: Пушкина, Лермонтова и Тютчева. Код реализует два этапа:

1. **Получение ссылок на стихотворения автора**:

def get\_poem\_links(author\_page\_url):

...

Этот блок обходит страницу автора, собирает все ссылки на отдельные стихотворения и возвращает их список.

2. **Извлечение текста стихотворения**:

def extract\_poem\_text(poem\_url):

...

Функция загружает страницу стихотворения, извлекает строки текста с помощью BeautifulSoup, очищает их от тегов и формирует список строк.

3. **Скачивание всего корпуса автора**:

def download\_author\_corpus(index\_url):

...

Проходимся по всем ссылкам на стихотворения, объединяем текст и сохраняем в список. Для замедления запросов используется time.sleep(0.8).

4. **Сохранение корпуса в файлы**:

for (path, text) in data:

`    `corpus = "\n\n".join(text)

`    `with open(path, "w", encoding="utf-8") as f:

`        `f.write(corpus)

Каждому автору соответствует свой текстовый файл.
### **3.2 Токенизация текста с помощью BPE**
Чтобы текст можно было подать в нейросеть, его необходимо представить в виде последовательности чисел (токенов). Для этого используется **Byte Pair Encoding (BPE)** через библиотеку SentencePiece.

spm.SentencePieceTrainer.train(

`    `input="/tmp/corpus.txt",

`    `model\_prefix="poetry",

`    `vocab\_size=2048,

`    `model\_type="bpe",

...

`    `user\_defined\_symbols=["\n"],  # Сохраняем перенос строки как отдельный токен

)

- **vocab\_size=2048** — словарь небольшой, но достаточный для tiny-GPT.
- **user\_defined\_symbols=["\n"]** — важно для поэзии, чтобы модель умела различать строки стихотворения.

После обучения токенизатор загружается и проверяется на примере:

test = "Я вас любил: любовь еще, быть может,\nНо пусть она вас больше не тревожит;"

print(sp.encode(test, out\_type=str))

print(sp.decode(sp.encode(test)))
###
### **3.3 Подготовка данных для модели**
Стихотворения разделяются по двойному \n\n, добавляются специальные токены <s> и </s> для начала и конца последовательности:

poems = [p.strip() for p in text.strip().split("\n\n") if p.strip()]

encoded = []

for poem in poems:

`    `ids = sp.encode(poem, add\_bos=True, add\_eos=True)

`    `encoded.extend(ids)

data = torch.tensor(encoded, dtype=torch.long)

data — это один длинный массив токенов, из которого будут выбираться батчи для обучения.
### **3.4 Реализация TinyGPT**
TinyGPT — это компактная версия Transformer с несколькими слоями. Основные блоки:

1. **Self-Attention с маской** (Head):

wei = q @ k.transpose(-2, -1) \* C\*\*-0.5

wei = wei.masked\_fill(self.tril[:T, :T] == 0, float('-inf'))

wei = F.softmax(wei, dim=-1)

Маска предотвращает использование будущих токенов при предсказании текущего.

2. **Multi-Head Attention**:

out = torch.cat([h(x) for h in self.heads], dim=-1)

Несколько голов attention позволяют модели учитывать разные аспекты последовательности.

3. **Feed Forward Network**:

self.net = nn.Sequential(

`    `nn.Linear(n\_embd, 4 \* n\_embd),

`    `nn.ReLU(),

`    `nn.Linear(4 \* n\_embd, n\_embd),

)

Добавляет нелинейность и повышает выразительную способность модели.

4. **Блок Transformer и Residual + LayerNorm**:

x = x + self.sa(self.ln1(x))

x = x + self.ffwd(self.ln2(x))

Обеспечивает стабильность обучения и более глубокие зависимости.

5. **Генерация текста**:

def generate(self, idx, max\_new\_tokens, temperature=1.0, top\_k=None):

...

Автрорегрессионно предсказываем следующий токен, применяя temperature scaling и top-k sampling для разнообразия и контроля качества текста.
### **3.5 Обучение модели**
Данные делятся на train/val 90/10. Батчи выбираются случайно:

def get\_batch(split):

`    `ix = torch.randint(len(data\_split) - block\_size, (batch\_size,))

`    `x = torch.stack([data\_split[i:i+block\_size] for i in ix])

- **Loss** — кросс-энтропия между предсказанными токенами и истинными.
- **Optimizer** — AdamW, learning rate 3e-4.
- **Early Stopping** — остановка обучения при отсутствии улучшения perplexity на валидации.
### **3.6 Сохранение модели и токенизатора**
После обучения модель сохраняется в Google Drive, вместе с токенизатором:

torch.save({'model\_state\_dict': model.state\_dict(), 'vocab\_size': vocab\_size, 'block\_size': block\_size}, model\_path)

!cp poetry.model poetry.vocab /content/drive/MyDrive/Lab3/

Это позволяет подгружать модель для генерации текста без повторного обучения.





**4 Примеры генерации текста**

Сгенерированные стихи:

\==================================================

1\. Промпт: "Я вас любил:"

\----------------------------------------

Я вас любил:

Горелкеанди,

Да ропали, где я тебя не любора.

Как счастье нашеться,

Как этих трепещу им под шкует;

Мы все знаетедайния,

Как я с вами друг, не шеру нежно,

Невиденьем.

Межать, что за тобой,

Неясь!

Нарждать себя,

Мой дух, что лучший, что не в груди друг!

О, не посмане остат;

Когда свы разго,

Мнешь,

Сквозь на твое? что я никогда.

\==================================================

2\. Промпт: "У лукоморья дуб зелёный;"

\----------------------------------------

У лукоморья дуб зелёный;

И в горы у меня воспоминание поляну поникшею на ваша

Приди людьми в Царен долина,

Понов веселых берегун,

При свете вечные уданских пира,

За,

Прозрачный паук изве и птиц.

Так спяется,

Когда возник молча въез парус главует,

За, поторагливают стремглали;

И вдруг собзали бы,

Я выражка и вновь приход.

Так проскак всегда мне главу,

Межа, по пого остановится, и

\==================================================

3\. Промпт: "Люблю грозу в начале мая,"

\----------------------------------------

Люблю грозу в начале мая,

Знакомый пробилаг,

Какое ли за ним, наблося набек

И опора

И мали леткий обпясь,

С горы наш речины дым,

И мни-де к небес замкой

Отарче колыбельмой

Стится ль гор,

` `Бающие хважный,

Святы мерпка острой зелезит,

Все на нем;

И. Они отдит над рекались,

Куда,

Все полно по горахой-пи,

Еще вы.

Но смолк,

То было хладный з

\==================================================

4\. Промпт: "Как хорошо ты, о, природа!"

\----------------------------------------

Как хорошо ты, о, природа!

Почеринью запла, с душой

Придила,

Где теньлажены резраться с той поры бесов,

Зачем неразрайне было

И возвы,

Бездворную лица лучше разговори

И я боюсь востоговоренье -

Изможно страхашего,

Оставлянул, как вольную том,

И слезы хранияго так ищетночи,

Изгнан

За все, как вихрьмеян над бездной припи

Тречь.

Из сбиваться,

Обнялся взглядомню

\==================================================

5\. Промпт: "Ночь. Улица. Фонарь."

\----------------------------------------

Ночь. Улица. Фонарь. Рель;

Ут, как боровод.

Пушустился и в горы,

Пономыслах сноваленой.

Отражба,

Ностройдут веет на нем трепет ожив тучи,

I

И снова привольный леса,

И нарядаль волны он в тьме лесов,

И шашутки

Как, как мывенного стак в волною

Вох замечный,

Смешлимотре;

И девалися перед гром,

И как черне;

Все, как птица приле, ее по кружит путника ри -

Но той

\==================================================

**5 Анализ графиков Loss и Perplexity**

`	`В ходе обучения модели tiny-GPT на корпусе русской поэзии были построены графики Loss и Perplexity (PPL) на обучении и валидации.



Рисунок 1 – графики Loss и Perplexity

**1. График Loss**

На графике слева показано, как меняется функция потерь на тренировочных и валидационных данных.

- **Начало обучения:** значение Loss примерно одинаково для обучения и валидации (~7.8), что соответствует случайным предсказаниям модели.
- **Быстрое падение в начале:** в первые 500 итераций Loss уменьшается до ~4.5 на тренировочных данных и до ~5.0 на валидации. Это означает, что модель быстро усвоила частотные закономерности корпуса и научилась предсказывать наиболее вероятные токены.
- **Стабилизация и лёгкий рост Val Loss после 1000 итераций:** тренировочный Loss продолжает снижаться, а Loss на валидации почти перестаёт улучшаться и даже немного растёт. Это свидетельствует о **начале переобучения**: модель хорошо запоминает тренировочные данные, но хуже обобщает на новые стихотворения.

**2. График Perplexity (PPL)**

На графике справа показано изменение perplexity — метрики, показывающей «удивление» модели при предсказании следующего токена.

- **Начало обучения:** высокая perplexity (~1200) соответствует полной неосведомлённости модели о корпусе.
- **Резкое снижение в начале:** в первые 500–1000 итераций PPL падает до ~100 на тренировке и ~150 на валидации. Модель научилась правильно предсказывать частые слова и токены.
- **Разрыв между Train и Val:** после ~1000 итераций PPL на тренировке продолжает снижаться до ~20–30, тогда как на валидации остаётся на уровне ~140–150. Это чётко показывает **переобучение**: модель слишком хорошо запомнила тренировочные данные, но не умеет обобщать на новые стихи.

**6 Анализ ошибок генерации**

При генерации текстов модель демонстрирует определённые ошибки, которые можно разделить на несколько категорий. Наиболее заметными являются искажения слов, появление несуществующих форм и нарушений грамматической структуры. Например, в сгенерированных стихах встречаются формы вроде «сплив с перловится» или «вы́рские конюрыгаясь», которые не существуют в русском языке. Эти ошибки объясняются как ограничениями корпуса данных, так и параметрами модели.

Во-первых, объём корпуса относительно мал по сравнению с современными языковыми моделями. Модель обучалась на нескольких сотнях стихотворений, что недостаточно для полного усвоения морфологических и синтаксических правил русского языка. Малая представленность различных лексических и грамматических конструкций приводит к тому, что модель пытается «собирать» слова по шаблонам из частотных фрагментов, иногда создавая новые или искажённые формы. Это особенно заметно в поэзии, где слова часто употребляются в нестандартных формах и сочетаниях.

Во-вторых, архитектура модели является упрощённой. Модель содержит лишь три слоя трансформеров, четыре головы внимания и ограниченное размерное скрытое пространство. Такие параметры ограничивают способность модели учитывать долгосрочные зависимости в тексте и усложняют моделирование синтаксических и ритмических структур, характерных для поэзии. В результате строки могут терять смысловую связь, особенно если генерация выходит за предел block size, когда внимание не охватывает предыдущие токены полностью.

В-третьих, ошибки генерации связаны с особенностями алгоритма обучения. Размер батча и количество итераций были ограничены для сокращения времени обучения, что привело к неполному усвоению распределений токенов. Также модель обучалась с относительно высокой скоростью обучения, что могло способствовать некоторой нестабильности градиентов и появлению шумов в распределении вероятностей токенов. Параметры top-k и temperature при генерации дополнительно влияют на разнообразие текста: низкая температура делает текст слишком предсказуемым и однообразным, высокая температура — увеличивает вероятность появления некорректных слов и несуществующих форм.

С точки зрения структуры текста модель демонстрирует положительные свойства. Несмотря на отдельные ошибки, она умеет сохранять формат стихотворной строки, использовать переносы и символы конца строки, а также имитировать ритм и повторяемость образов, характерных для русского стиха. Тем не менее, смысловая связность отдельных фрагментов остаётся ограниченной, и модель часто создаёт бессмысленные сочетания слов.

В целом, ошибки генерации являются следствием сочетания ограниченного корпуса данных, упрощённой архитектуры модели и выбранных параметров обучения. Увеличение объёма корпуса, расширение числа слоёв трансформеров и увеличение скрытого пространства, а также более точная настройка параметров обучения и генерации могли бы значительно снизить количество подобных ошибок и улучшить когерентность сгенерированных текстов.


**Использованные источники**

1\. Mikolov T., Chen K., Corrado G., Dean J. Efficient Estimation of Word Representations in Vector Space [Электронный ресурс]. — 2013. — URL: https://arxiv.org/abs/1301.3781 (дата обращения: 30.12.2025).

2\. Jurafsky D., Martin J. H. *Speech and Language Processing: Chapter 3. N-gram Language Models* [Электронный ресурс]. — Stanford University, 2025. — URL: [https://web.stanford.edu/~jurafsky/slp3/3.pdf](https://web.stanford.edu/~jurafsky/slp3/3.pdf?utm_source=chatgpt.com) (дата обращения: 30.12.2025). 

3\. Hugging Face. *Byte-Pair Encoding tokenization — Hugging Face LLM Course* [Электронный ресурс]. — URL: [https://huggingface.co/learn/llm-course/en/chapter6/5](https://huggingface.co/learn/llm-course/en/chapter6/5?utm_source=chatgpt.com) (дата обращения: 30.12.2025). [Hugging Face](https://huggingface.co/learn/llm-course/en/chapter6/5?utm_source=chatgpt.com)

4\. Hugging Face. *Transformers, what can they do? — Hugging Face LLM Course* [Электронный ресурс]. — URL: [https://huggingface.co/learn/llm-course/en/chapter1/3](https://huggingface.co/learn/llm-course/en/chapter1/3?utm_source=chatgpt.com) (дата обращения: 30.12.2025). [Hugging Face](https://huggingface.co/learn/llm-course/en/chapter1/3?utm_source=chatgpt.com)

5\. Максимов М. *Что такое эмбеддинги и как с ними работать. Вводная для начинающих* [Электронный ресурс]. Habr.com, 15 сентября 2021 г. — URL: [https://habr.com/ru/articles/947216/](https://habr.com/ru/articles/947216/?utm_source=chatgpt.com) (дата обращения: 30.12.2025).

